import os
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import Chroma
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
from langchain_community.llms import Ollama
from langchain.docstore.document import Document
from app.config import CHROMA_DB_PARENT_PATH, EMBEDDING_MODEL, OLLAMA_MODEL


def get_rag_chain(query: str):
    print("üöÄ Initializing RAG Chain...")
    
    embedding_function = HuggingFaceEmbeddings(
        model_name=EMBEDDING_MODEL,
        model_kwargs={"trust_remote_code": True}
    )

    all_docs = []
    
    print(f"üìÅ Scanning Chroma vector stores under: {CHROMA_DB_PARENT_PATH}")
    for subdir in os.listdir(CHROMA_DB_PARENT_PATH):
        db_path = os.path.join(CHROMA_DB_PARENT_PATH, subdir)
        if os.path.isdir(db_path):
            try:
                print(f"üîç Loading vector store: {subdir}")
                vs = Chroma(
                    persist_directory=db_path,
                    embedding_function=embedding_function
                )

                docs = vs.similarity_search_with_score(query, k=5)  # Now using real query

                print(f"   üì¶ Retrieved {len(docs)} chunks from '{subdir}'")
                for doc, score in docs:
                    doc.metadata["source"] = subdir
                    doc.metadata["score"] = score
                    all_docs.append(doc)
            except Exception as e:
                print(f"‚ùå Failed to load or query DB at '{db_path}': {e}")

    if not all_docs:
        raise ValueError("Aucun document pertinent trouv√© dans les sous-dossiers de ChromaDB.")

    # Sort and take top 5
    all_docs = sorted(all_docs, key=lambda x: x.metadata.get("score", 1.0))[:5]

    print("\n‚úÖ Final Top 5 Chunks Selected:")
    for i, doc in enumerate(all_docs, 1):
        source = doc.metadata.get("source", "unknown")
        score = doc.metadata.get("score", "n/a")
        preview = doc.page_content[:100].replace('\n', ' ')
        print(f"  {i}. üìÑ From '{source}' | Score: {score:.4f} | Content: {preview}...")

    # Build temporary in-memory Chroma DB
    temp_db = Chroma.from_documents(documents=all_docs, embedding=embedding_function)

    prompt = PromptTemplate(
        input_variables=["context", "question"],
        template=(
            "Vous √™tes un assistant utile. R√©pondez √† la question suivante en utilisant uniquement le contexte fourni. "
            "R√©pondez dans la m√™me langue que la question. Si le contexte est insuffisant pour r√©pondre, dites simplement : "
            "'je ne sais pas'.\n"
            "Contexte : {context}\n"
            "Question : {question}\n"
            "R√©ponse :"
        )
    )


    prompt = PromptTemplate(
        input_variables=["context", "question"],
        template=(
            "Vous √™tes un assistant qui r√©pond √† des questions en extrayant des informations pr√©cises du contexte.\n\n"
            "üìå Utilisez uniquement les informations fournies dans le contexte.\n"
            "üìå Si la question concerne un contact, un nom ou un email, extrayez-les clairement.\n"
            "üìå Si aucune information pertinente n‚Äôest disponible, r√©pondez : ¬´ je ne sais pas ¬ª.\n\n"
            "Contexte :\n{context}\n\n"
            "Question :\n{question}\n\n"
            "R√©ponse :"
        )
    )


    chain = RetrievalQA.from_chain_type(
        llm = Ollama(model=OLLAMA_MODEL, temperature=0.2, timeout=90),
        retriever=temp_db.as_retriever(),
        chain_type="stuff",
        return_source_documents=True,
        chain_type_kwargs={"prompt": prompt},
        verbose=True
    )

    print("‚úÖ RAG Chain ready.\n")
    return chain
